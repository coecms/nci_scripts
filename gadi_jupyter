#!/bin/bash

# Scott Wales 20190522

print_help() {
cat <<EOF
Run a Jupyter notebook on Gadi's compute nodes, presenting the interface in a
browser on the local machine

General Options:
    -h:         Print help
    -l:         NCI username
    -L:         NCI login node (default 'gadi.nci.org.au')
    -e:         Conda environment

Queue Options:
    -q QUEUE:     Queue name
    -n NCPU:      Use NCPU cpus
    -m MEM:       Memory allocation (default 4*NCPU GB)
    -t TIME:      Walltime limit (default 1 hour)
    -J JOBFS:     Jobfs allocation (default 100 GB)
    -P PROJ:      Submit job under project PROJ
    -R RECONNECT: Reconnect to existing job under project PROJ 
                  (Boolean: default True)
EOF
}

set -eu

# Internal defaults
USER=''
PROJECT=''
LOGINNODE='gadi.nci.org.au'
QUEUE='normal'
NCPUS='1'
MEM=''
WALLTIME=1:00:00
JOBFS=100gb
CONDA_ENV=analysis3-20.01
RECONNECT=True

# Handle arguments
optspec="hl:L:q:n:m:t:J:P:e:R:"
while getopts "$optspec" optchar; do
    case "${optchar}" in
        h)
            print_help
            exit 2
            ;;
        l)
            USER="${OPTARG}"
            ;;
        L)
            LOGINNODE="${OPTARG}"
            ;;
        q)
            QUEUE="${OPTARG}"
            ;;
        n)
            NCPUS="${OPTARG}"
            ;;
        m)
            MEM="${OPTARG}"
            ;;
        t)
            WALLTIME="${OPTARG}"
            ;;
        J)
            JOBFS="${OPTARG}"
            ;;
        P)
            PROJECT="${OPTARG}"
            ;;
        e)
            CONDA_ENV="${OPTARG}"
            ;;
	R)  RECONNECT="${OPTARG}"
            ;;
        *)
            print_help
            exit 2
            ;;
    esac
done

# This gets evaluated on Gadi in the SSH script
WORKDIR=\$TMPDIR/runjp

SSH='ssh -oBatchMode=yes'
if [ -n "$USER" ]; then
    SSH="${SSH} -l ${USER}"
fi
if [ -z "$MEM" ]; then
    MEM="$(( NCPUS * 4 ))gb"
fi

SUBMITOPTS="-N jupyter-notebook ${PROJECT:+-P '$PROJECT'} -q '$QUEUE' -l 'ncpus=${NCPUS},mem=${MEM},walltime=${WALLTIME},jobfs=${JOBFS}'"

# Check connection
$SSH "$LOGINNODE" true

echo "qsub ${SUBMITOPTS}"

# Kill the job if this top-level script is cancelled while the job is still in the queue
trap "{ echo 'Stopping queued job... (Ctrl-C will leave job in the queue)' ; $SSH \"$LOGINNODE\" <<< \"qdel \\\$(cat \\$WORKDIR/jobid)\" ; }" EXIT

if $RECONNECT; then

message=$(
$SSH -q "$LOGINNODE" <<EOF | tail -n 1
PROJECT="$PROJECT"
# Look for processes titled jupyter-notebook. 
processes=(\$(/g/data/hh5/public/apps/nci_scripts/uqstat -f csv  | grep jupyter-notebook | grep \$PROJECT | sed 's/,/\n/g'))
if [[ -n \$processes ]];
then
    set -eu
    # Count number of jupyter-notebook sessions.
    repeat=(\$( echo \${processes[@]} | tr " " "\n" | sort | uniq -c | sort -k2nr | grep jupyter-notebook | sed -e 's/^ *//g' | tr " " "\n" ))
    # TO DO: Perhaps join all sessions or kill all processes. ATM, it will connect 
    # to first session.
    if [[ \$repeat != "1" ]]; 
    then
        echo "There are several sessions running"; 
        qcat \${processes[0]} | head -n 1
    else
        # Extract process information
        qcat \${processes[0]} | head -n 1
    fi
else
    # If job doesn't exist then the message is overwritten
    echo "New Job"
fi
EOF
)
else
    message="New Job"
fi

if [[ ! $RECONNECT ]] || [[ $message == "New Job" ]]
then

echo "Starting notebook on ${LOGINNODE}..."

message=$(
$SSH -q "$LOGINNODE" <<EOF | tail -n 1

set -eu

WORKDIR="$WORKDIR"
mkdir -p "\$WORKDIR"
rm -f "\$WORKDIR/message"

cat > "\$WORKDIR/runjp.sh" <<EOQ
#!/bin/bash

module purge

eval "\\\$(/g/data/hh5/public/apps/miniconda3/bin/conda shell.bash hook)"
conda activate "${CONDA_ENV}"

set -eu

# Jupyter security token
TOKEN=\\\$(uuidgen)

# Find a remote port https://unix.stackexchange.com/a/132524
PORT=\\\$(python -c 'import socket; s=socket.socket(); s.bind(("", 0)); print(s.getsockname()[1]); s.close()')

# Write message file with info for the local connection
echo "\\\$HOSTNAME \\\$TOKEN \\\$PBS_JOBID \\\$PORT" > "\$WORKDIR/message"

cat "\$WORKDIR/message"

export DASK_LABEXTENSION__FACTORY__MODULE=dask.distributed
export DASK_LABEXTENSION__FACTORY__CLASS=LocalCluster
export DASK_LABEXTENSION__FACTORY__KWARGS__MEMORY_LIMIT=3900MB
export DASK_LABEXTENSION__FACTORY__KWARGS__LOCAL_DIRECTORY=\\\$PBS_JOBFS/dask-worker-space
export DASK_LABEXTENSION__DEFAULT__WORKERS=\\\$PBS_NCPUS
export DASK_DISTRIBUTED__DASHBOARD__LINK="/proxy/{port}/status"

jupyter notebook --NotebookApp.token="\\\$TOKEN" --no-browser --ip="\\\$HOSTNAME" --port "\\\$PORT" --port-retries=0
EOQ

storage="gdata/hh5"
for p in \$(id -nG); do
    if [ \$p = "access.dev" ]; then continue; fi
    if [ \$p = "access.admin" ]; then continue; fi
    if [ -d "/scratch/\$p" ]; then storage="scratch/\$p+\$storage"; fi
    if [ -d "/g/data/\$p" ]; then storage="gdata/\$p+\$storage"; fi
done

qsub $SUBMITOPTS -l "storage=\$storage" -j oe -o "\$WORKDIR/pbs.log" "\$WORKDIR/runjp.sh" > "\$WORKDIR/jobid"

# Wait for the message file to appear, then return to the local process
while [ ! -f "\$WORKDIR/message" ]; do
    sleep 5
done
cat "\$WORKDIR/message"
EOF
)
else
echo "Reconnecting to notebook on ${LOGINNODE}..."
fi

echo "Remote Message: '$message'"

# Grab info from the PBS job
read jobhost token jobid remote_port <<< "$message"

# Find a local port
for local_port in {8888..9000}; do
    if ! echo > /dev/tcp/127.0.0.1/${local_port} ; then
        break
    fi 2> /dev/null
done

echo "Notebook running as PBS job ${jobid}"
echo
echo "Starting tunnel..."
$SSH -N -L "${local_port}:$jobhost:${remote_port}" "$LOGINNODE" &
tunnelid=$!

# Shut everything down on exit
trap "{ echo 'Closing connections... (Ctrl-C will leave job in the queue)' ; kill $tunnelid ; $SSH "$LOGINNODE" qdel $jobid ; }" EXIT

# Wait for startup then open a browser
sleep 5
URL="http://localhost:${local_port}/lab?token=${token}"

cat << EOF

Start a Dask cluster in your notebook using the Dask panel of Jupyterlab, or by
running

---------------------------------------------------------------
import os
import dask.distributed

# Edit as desired
threads_per_worker = 1

try:
    c # Already running
except NameError:
    c = dask.distributed.Client(
        n_workers=int(os.environ['PBS_NCPUS'])//threads_per_worker,
        threads_per_worker=threads_per_worker,
        memory_limit=f'{3.9*threads_per_worker}gb',
        local_directory=os.path.join(os.environ['PBS_JOBFS'],
                                     'dask-worker-space')
    )
c
---------------------------------------------------------------

Opening ${URL}
EOF

set +e
if which xdg-open > /dev/null; then
    xdg-open "$URL"
elif which open > /dev/null; then
    open "$URL"
elif which explorer > /dev/null; then
    explorer "$URL"
else
    echo
    echo "----"
    echo "Notebook available at $URL"
fi
set -e

# Keep open as long as the tunnel exists
wait "$tunnelid"
